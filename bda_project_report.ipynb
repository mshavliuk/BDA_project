{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E5710 BAYESIAN DATA ANALYSIS - PROJECT\n",
    "## Table of Content:\n",
    "* [1. INTRODUCTION](#1)\n",
    "* [2. DESCRIPTION OF DATA AND THE ANALYSIS PROBLEM](#2)\n",
    "* [3. MODELS' DESCRIPTION](#3)\n",
    "* [4. MODEL COMPARSION](#4)\n",
    "* [5. PREDICTIVE PERFORMANCE ASSESSMENT](#5)\n",
    "* [6. DISCUSSION AND POTENTIAL IMPROVEMENTS](#6)\n",
    "* [7. CONCLUSION](#7)\n",
    "* [8. SELF-REFLECTION LESSONS](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION<a class=\"anchor\" id=\"1\"></a>\n",
    "Predicting one's heart condition is crucial for giving proper medical decisions and possibly saving lives. The misdiagnosis of heart disease may cause serious problems, as being left untreated adequately can even lead to an irreversible damage in the heart muscle and can be life-threatening. Therefore, the correct diagnosis of the heart disease status is extremely vital not only for patients’ health but also their live. Unfortunately, the heart disease status cannot be diagnosed easily since there are many blood vessels in the human body which lead directly to the heart and it is very expensive and time consuming to check all the blood vessels’ conditions by imaging method such as coronary angiogram. Hence, before making the decision whether or not conducting complicated examination techniques, it is important for the doctor to accurately assess the patients’ heart condition based on easily measurable biometric parameters such as age, sex, chest paint types, resting blood vessels, blood cholesterol levels, etc.\n",
    "\n",
    "The report is divided into ... different sections:\n",
    "\n",
    "• **Section 1** introduces the application domain of the Heart Disease Detection Bayesian problem\n",
    "\n",
    "• **Section 2** explains how the Bayesian problem is formulated. The aim of this section is to define what data points represent and which properties of a data point are used as its features and labels.\n",
    "\n",
    "• **Section 3** visualizes the data and relation between the features and the labels\n",
    "• ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DESCRIPTION OF DATA AND THE ANALYSIS PROBLEM<a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. age\n",
    "2. sex: 0: female, 1: male\n",
    "3. cp: chest pain type (4 values) 1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic\n",
    "4. trestbps: resting blood pressure\n",
    "5. chol: serum cholestoral in mg/dl\n",
    "6. fbs: fasting blood sugar > 120 mg/dl (1 = True, 0 = False)\n",
    "7. restecg: resting electrocardiographic results (values 0 = normal, 1 = having ST-T wave abnormality, 2 = showing proable or definite left ventricular hypertrophy by Estes's criteria)\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "12. ca: number of major vessels (0-4) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "14. target: angiographic disease status (0 = <50% diameter narrowing _ no presence, 1% = >50% diameter narrowing _ presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.952197</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.763956</td>\n",
       "      <td>-0.256334</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015443</td>\n",
       "      <td>0</td>\n",
       "      <td>1.087338</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.915313</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.092738</td>\n",
       "      <td>0.072199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.633471</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122573</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.474158</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.092738</td>\n",
       "      <td>-0.816773</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977514</td>\n",
       "      <td>0</td>\n",
       "      <td>0.310912</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.180175</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.663867</td>\n",
       "      <td>-0.198357</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.239897</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.206705</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.290464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.663867</td>\n",
       "      <td>2.082050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.583939</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.379244</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.952197    1   3  0.763956 -0.256334    1        0  0.015443      0   \n",
       "1 -1.915313    1   2 -0.092738  0.072199    0        1  1.633471      0   \n",
       "2 -1.474158    0   1 -0.092738 -0.816773    0        0  0.977514      0   \n",
       "3  0.180175    1   1 -0.663867 -0.198357    0        1  1.239897      0   \n",
       "4  0.290464    0   0 -0.663867  2.082050    0        1  0.583939      1   \n",
       "\n",
       "    oldpeak  slope  ca  thal  target  \n",
       "0  1.087338      0   0     1       1  \n",
       "1  2.122573      0   0     2       1  \n",
       "2  0.310912      2   0     2       1  \n",
       "3 -0.206705      2   0     2       1  \n",
       "4 -0.379244      2   0     2       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Normalizing numeric features value\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS = StandardScaler()\n",
    "col_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "df[col_to_scale] = SS.fit_transform(df[col_to_scale])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our application can be modelled as Bayesian problem with **data points** representing patients who have already undergone heart tests. Each data point is characterized by 13 different health parameters such as age, sex, chest pain type, resting blood pressure, etc. The **label** (quantity of interest) of a data point is the heart disease status, for which values 0 and 1 indicate no presence and presence of a heart disease, respectively.\n",
    "We gathered the data points with known label values using the patients’ health data recording available from UC Irvine machine learning heart disease repository which can be accessed via the link: https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\n",
    "\n",
    "The above data repository contains health records data from 4 different locations such as Cleveland and Long Beach of America, Switzerland, and Hungary. However, only the Cleveland’s dataset is in a good condition and being maintained. All other datasets are in poor condition and do have a lot of missing information. Thus, we will use the Cleveland dataset which consists of 303 data points which are suitable for our study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODELS' DESCRIPTION<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "\n",
    "import inspect\n",
    "\n",
    "import arviz as az\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "import linear_regression as lin\n",
    "\n",
    "data = pd.read_csv('heart.csv')\n",
    "samples = data[data.columns.difference(['target'])]\n",
    "outcomes = data['target']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model description\n",
    "\n",
    "The multivariable linear regression model describes the relation of given parameters to the probability of detecting disease.\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta_i = \\alpha_1 + \\beta_1x_{1i} + \\alpha_2 + \\beta_2x_{2i} + ... + \\alpha_J + \\beta_Jx_{Ji}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "- $\\theta_i$ is the probability of detecting disease\n",
    "- $\\alpha_.$ and $\\beta_.$ is the intercept and scalar response parameters of $\\beta{x}+\\alpha$\n",
    "- $J$ is the number of studied variables or number of dimensions from given data\n",
    "\n",
    "Stan model embeds the necessary data normalization and linearly scales $x_.$ to range $[0;1]$.\n",
    "\n",
    "Spliting $\\alpha$ into $J$ parts is rather unconventional, but it allows to better visualise correlation for each parameter of interest separately.\n",
    "\n",
    "Thus, it gives a set of separate $kx + b$ -like equations that are easy to visualise and validate based on expert knowledge.\n",
    "\n",
    "See the related Stan model source code in [Appendix A](#Appendix-A).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "nest_asyncio.apply()\n",
    "model = lin.build(samples, outcomes)\n",
    "fit = lin.sample(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the visualisation of the linear correlation of disease probability and parameter values shown in red lines. Blue spots show the distribution of given data, jittered along the x-axis to improve readability. y-axis illustrates the estimated probability of disease.\n",
    "\n",
    "TODO:2 mention that we are excluding the non-significant factors from further analysis to get better elpd values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin.plot_draws(fit, samples)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convergence diagnostics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import diagnostics\n",
    "\n",
    "diagnostics.convergence(fit, var_names=['alpha', 'beta', 'sigma'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Based on `r_hat` values, all chains have successfully converged for all estimated variables. None of them exceeds the set threshold value $1.01$.\n",
    "\n",
    "TODO:1 interpret ess, n_eff, tree depth and divergences\n",
    "\n",
    "#### Loo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diagnostics.psis_loo_summary(fit, 'Linear')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen above, the PSIS-LOO doesn't provide reliable estimate due to a large number of Pareto-K values higher than 1 (mean K value is about 3 times higher). Due to that fact can need run K-Fold validation to get more reliable predictive accuracy estimates and provide some intuition about an actual predictive accuracy.\n",
    "\n",
    "The source code of `k_fold_cv` function can be found in  [Appendix C](#Appendix-C).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diagnostics.k_fold_cv(lin.build, lin.get_disease_prob, samples, outcomes, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen, the linear model is not able to properly describe the data and give reliable estimates. It has low predictive accuracy a bit higher than 80% and produces especially high number of false positives diagnoses.\n",
    "\n",
    "One way of improving the model could be changing its sensitivity threshold, but in best case scenario it improved the model accuracy just by a couple percents, likely just by chance. This approach is far from perfect and doesn't fix false positive predictions' error.\n",
    "\n",
    "Hence, the linear regression is not recommended to use in real applications, but it can reveal basic insights on the underlying correlations. The next chapter consider the logistic regression model which is expected to show better results.\n",
    "\n",
    "TODO:2 Background knowledge check if the model make sense\n",
    "\n",
    "\n",
    "#### Prior sensitivity\n",
    "\n",
    "For the prior sensitivity analysis we will use a bit more simple and much quicker version of leave-one-out validation and check within-sample predictive accuracy. This summary is quick and easy to understand but is in general an overestimate of LOO-CV because it is evaluated on the data from which the model was fit.\n",
    "\n",
    "The PSIS-LOO values are also included in the analysis to build more solid connection between posterior log likelihood values and predictive accuracy.\n",
    "\n",
    "For the `loo_within_sample` implementation see [Appendix D](#Appendix-D).\n",
    "\n",
    "\n",
    "TODO:1 Sensitivity analysis with respect to prior choices (create sensitivity table)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic regression model\n",
    "\n",
    "#### 3.2.1. Model theoratical/mathematic description\n",
    "\n",
    "The multivariable logistic regression model describes the relation of given parameters to the probability of detecting disease:\n",
    "$$ Y_i|X_i \\sim Bernoulli(expit(\\alpha + \\beta_1  x_{1i} + ... + \\beta_J x_{Ji}))$$ \n",
    "$$ Log Odds(Y=1|X) = log\\bigg(\\frac{\\theta_i}{1-\\theta_i}\\bigg) = \\alpha + \\alpha + \\beta_1  x_{1i} + ... + \\beta_J x_{Ji}$$\n",
    "$$ i = 1..N$$\n",
    "where\n",
    "\n",
    "• $\\theta_i$ is the probability of detecting disease\\\n",
    "• $\\alpha$ and $\\beta_k$ is the parameters of the logistic regression model\\\n",
    "• J is the number of features and N is the number of data points\\\n",
    "\n",
    "#### 3.2.2. Priors choice explaination\n",
    "\n",
    "Although the heart disease dataset was published long time ago, there is still limited amount of medical research using it and the researches study about the relation between the parameters such  ST depression induced by exercise relative to rest or maximum heart rate achieve to the heart disease status. Therefore, we think using the previous knowledge from other heart disease cases is almost imposible for use, instead we used weakly informative priors. We have tried out different priors options and checked out the accuracy of the model. At the end, we decided to choose student distribution _ student_t(2,0,10) as a default prior for alpha and normal distribution _ normal(0, 10) as the default prior for beta.\n",
    "\n",
    "#### 3.2.3. Model stan code\n",
    "\n",
    "Below is the stan code for logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = '''\n",
    "data {\n",
    "    int<lower=1> N;                                     // number of data points\n",
    "    int<lower=1> J;                                     // number of dimensions\n",
    "    matrix[N, J] X;                                     // model input data    \n",
    "    int<lower=0, upper=1> y[N];                         // outcomes\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    real alpha;        // intercept\n",
    "    vector[J] beta;    // regression coefficient\n",
    "}\n",
    "\n",
    "model {\n",
    "    // Prior\n",
    "    alpha ~ student_t(2, 0, 10);\n",
    "    beta ~ normal(0, 1);\n",
    "\n",
    "    // Likelihood / distribution of y\n",
    "    y ~ bernoulli_logit(alpha + X * beta);\n",
    "}\n",
    "generated quantities {     \n",
    "    real<lower=0, upper=1> probs[N];    \n",
    "    vector[N] log_lik = rep_vector(0, N);    \n",
    "    real tmp;\n",
    "\n",
    "    // Calculate LOO\n",
    "    for (i in 1:N)\n",
    "    {\n",
    "        tmp = 0;\n",
    "        for (j in 1:J)\n",
    "        {\n",
    "            tmp += beta[j] * X[i,j];\n",
    "        }                \n",
    "        log_lik[i] = bernoulli_logit_lpmf(y|alpha + tmp);\n",
    "        probs[i] = inv_logit(alpha + tmp); // model\n",
    "    }         \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4. Convergence diagnostics (Rhat, ESS), HMC specific convergence diagnostics (divergences, tree depth)\n",
    "Below image shows model building result\n",
    "![title](img/logistic_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above picture\n",
    "\n",
    "• All the parameters's $\\hat{R}$ value is close to 1; therefore, the generated chains have been converged well.\n",
    "\n",
    "• The ESS value for the model is more than 1000, this indicates that the models will have stable estimations for most applications.\n",
    "\n",
    "TODO tree_depth, diverging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5. Posterior predictive checks and what was done to improve the model\n",
    "\n",
    "#### 3.2.6. Sensitivity analysis with respect to the prior choices\n",
    "For the sensitivity check we have tried to build the model with different options for beta distribution such as uniform, normal and double exponential (Laplace) distribution. We want check that will output models (alpha and betas) are significantly changed when we vary the beta prior.\n",
    "By default the **alpha** is always drawn from **student_t(2, 0, 10)** distribution. The only exception is when the beta is drawn from uniform(-inf, inf), we also need to draw alpha from the same distribution (uniform(-inf, inf)); otherwise, the model build will be failed with any other different distribution for alpha.\n",
    "\n",
    "\n",
    "| beta distribution | uniform(-inf, inf) | normal(0,1) | normal(0,10) | normal(0,100) | double_exponential(0,1) |\n",
    "| --- | --- | --- |--- | --- |--- |\n",
    "| alpha| -0.258 |-0.233  |-0.255  |-0.252  |-0.288|\n",
    "| beta[0]| 0.964 |0.930  |0.971  |0.959  |0.922|\n",
    "| beta[1]| -0.348 |-0.330  |-0.352  |-0.341  |-0.310|\n",
    "| beta[2]| 0.576 |0.566  |0.571  |0.571  |0.579|\n",
    "| beta[3]| -0.795 |-0.768  |-0.801  |-0.791  |-0.764|\n",
    "| beta[4]| -0.998 |-0.963  |-1.004|-1.012  |-0.972|\n",
    "\n",
    "As shown in the table above, when we change the beta's prior to be drawn from multiple different distributions, the output model does not changed much, the alpha's and betas' number stay more or less the same. Therefore, we can conclude that **our logistic regression model is not sensitive to the choice of prior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MODEL COMPARSION<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PREDICTIVE PERFORMANCE ASSESSMENT<a class=\"anchor\" id=\"5\"></a>\n",
    "TODO:\n",
    "(e.g. classification accuracy) and evaluation of practical usefulness of the accuracy. If not applicable, then explanation why in this case the predictive performance is not applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DISCUSSION ISSUES AND POTENTIAL IMPROVEMENTS<a class=\"anchor\" id=\"6\"></a>\n",
    "- The linear and logistic regression models are one of the simpliest models that can be use to predict data. Using over simplify models exposes the thread that the model will fail to precisely predict the data. In future, we want to conduct more experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CONCLUSION<a class=\"anchor\" id=\"7\"></a>\n",
    "TODO: What was learn from the data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SELF-REFLECTION LESSONS<a class=\"anchor\" id=\"8\"></a>\n",
    "TODO: What group learn during project\n",
    "\n",
    "\n",
    "### Appendix A\n",
    "\n",
    "**Stan code for linear regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('linear_regression.stan', 'r') as file:\n",
    "    display_markdown(f\"```cpp\\n{file.read()}\\n```\", raw=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Appendix C\n",
    "\n",
    "**K-Fold validation**\n",
    "\n",
    "This function implements the K-Fold validation by fitting the model with n/k elements excluded from the data. Then estimated disease probability is then compared with actual `target` value for the given test sample."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_markdown(f\"```python\\n{inspect.getsource(lin.k_fold_cv)}\\n```\", raw=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Appendix D\n",
    "\n",
    "**Leave-one-out within-sample predictive accuracy**\n",
    "\n",
    "This function relies on `probs` values computed by Stan model's `generated quantities` block."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_markdown(f\"```python\\n{inspect.getsource(lin.loo_within_sample)}\\n```\", raw=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}